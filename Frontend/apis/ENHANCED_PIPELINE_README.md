# ğŸš€ Enhanced Data Privacy Pipeline with Configurable Scan Types

## Overview
The enhanced pipeline provides flexible, configurable data privacy scanning with support for different scan types, modular execution, and comprehensive SDE-based filtering. Users can now specify exactly what they want to scan and analyze.

## ğŸ¯ Pipeline Features

### **Scan Type Options**
- **Latest Database**: Scan only the most recently discovered database
- **All Databases**: Comprehensive scan of all client databases
- **Specific Database**: Targeted scan of a particular resource/database

### **Modular Execution**
- **Discovery Control**: Enable/disable data source discovery
- **AI Analysis Control**: Enable/disable AI-enhanced analysis
- **Table Filtering**: Specify particular tables to scan (for specific scans)

### **SDE Integration**
- All scan types automatically use client's selected SDEs for filtering
- Backward compatible with clients who haven't selected SDEs
- Real-time pattern filtering based on client preferences

---

## ğŸŒ Enhanced Pipeline Endpoint

### **`POST /run-pipeline`**
Execute complete data privacy pipeline with configurable scan options.

#### **Request Schema**
```json
{
  "client_id": "string",              // Required: Client identifier
  "scan_type": "latest|all|specific", // Required: Type of scan to perform
  "store_name": "string",             // Required if scan_type = "specific"
  "tables": ["string"],               // Optional: Specific tables to scan
  "include_discovery": boolean,       // Optional: Run discovery step (default: true)
  "include_ai_analysis": boolean      // Optional: Run AI analysis (default: true)
}
```

#### **Field Descriptions**
- **`client_id`**: Unique client identifier for multi-tenant operations
- **`scan_type`**: Determines scan scope - "latest", "all", or "specific"
- **`store_name`**: Target resource name (required for specific scans)
- **`tables`**: Array of table names to limit scan scope (optional)
- **`include_discovery`**: Whether to discover new sources before scanning
- **`include_ai_analysis`**: Whether to perform AI-enhanced analysis after scanning

---

## ğŸ“Š Pipeline Configuration Examples

### **1. Quick Latest Database Scan**
Fast scan of the most recently discovered database with full pipeline.

```json
POST /run-pipeline
{
  "client_id": "9LYPHynJIjR8c",
  "scan_type": "latest",
  "include_discovery": true,
  "include_ai_analysis": true
}
```

**Use Case**: Quick security check, daily monitoring, rapid assessment

---

### **2. Comprehensive All Databases Scan**
Complete scan of all client databases with AI analysis.

```json
POST /run-pipeline
{
  "client_id": "9LYPHynJIjR8c",
  "scan_type": "all",
  "include_discovery": false,
  "include_ai_analysis": true
}
```

**Use Case**: Compliance audit, comprehensive assessment, periodic full scan

---

### **3. Targeted Specific Database Scan**
Focus on one particular resource with optional table filtering.

```json
POST /run-pipeline
{
  "client_id": "9LYPHynJIjR8c",
  "scan_type": "specific",
  "store_name": "customer-data-bucket",
  "tables": ["users", "orders", "payments"],
  "include_discovery": true,
  "include_ai_analysis": true
}
```

**Use Case**: Incident investigation, new resource assessment, focused analysis

---

### **4. Discovery + Scan Only (No AI)**
Discover sources and scan without AI analysis for speed.

```json
POST /run-pipeline
{
  "client_id": "9LYPHynJIjR8c",
  "scan_type": "all",
  "include_discovery": true,
  "include_ai_analysis": false
}
```

**Use Case**: Quick inventory update, performance-focused scanning, batch processing

---

### **5. Scan-Only Mode (Skip Discovery)**
Scan existing known sources without discovery phase.

```json
POST /run-pipeline
{
  "client_id": "9LYPHynJIjR8c",
  "scan_type": "latest",
  "include_discovery": false,
  "include_ai_analysis": true
}
```

**Use Case**: Repeat scanning, known environment monitoring, automated workflows

---

## ğŸ“¤ Enhanced Pipeline Response

### **Success Response Schema**
```json
{
  "status": "success",
  "message": "Pipeline completed successfully with [scan_type] scan",
  "results": {
    "client_id": "string",
    "pipeline_config": {
      "scan_type": "string",
      "store_name": "string",
      "tables": ["string"],
      "include_discovery": boolean,
      "include_ai_analysis": boolean
    },
    "steps_completed": ["string"],
    "start_time": "ISO8601 timestamp",
    "discovery_results": { ... },      // If include_discovery = true
    "scan_results": { ... },
    "ai_analysis": { ... },            // If include_ai_analysis = true
    "end_time": "ISO8601 timestamp",
    "status": "completed",
    "total_steps": number
  },
  "timestamp": "ISO8601 timestamp"
}
```

### **Example Complete Response**
```json
{
  "status": "success",
  "message": "Pipeline completed successfully with specific scan",
  "results": {
    "client_id": "9LYPHynJIjR8c",
    "pipeline_config": {
      "scan_type": "specific",
      "store_name": "customer-data-bucket",
      "tables": ["users", "orders"],
      "include_discovery": true,
      "include_ai_analysis": true
    },
    "steps_completed": ["discovery", "scan_specific", "ai_analysis"],
    "start_time": "2025-08-03T10:30:00.000Z",
    "discovery_results": {
      "total_sources_discovered": 8,
      "new_sources": 2,
      "source_types": {
        "gcs": 3,
        "bigquery": 2,
        "postgresql": 2,
        "mysql": 1
      }
    },
    "scan_results": {
      "source_name": "customer-data-bucket",
      "source_type": "gcs",
      "total_findings": 25,
      "scan_duration": "45.2s",
      "tables_scanned": ["users", "orders"],
      "findings_by_sde": {
        "email_address": 12,
        "phone_number": 8,
        "credit_card_number": 5
      }
    },
    "ai_analysis": {
      "analysis_type": "enhanced_pipeline",
      "scan_type_analysis": "specific",
      "data_sensitivity_assessment": "Medium",
      "findings_summary": {
        "total_findings": 25,
        "scan_scope": "specific",
        "targeted_analysis": "customer-data-bucket"
      },
      "recommendations": [
        "Configure SDE filtering for specific scans",
        "Implement data classification based on findings",
        "Set up monitoring for high-risk data sources"
      ]
    },
    "end_time": "2025-08-03T10:32:15.000Z",
    "status": "completed",
    "total_steps": 3
  },
  "timestamp": "2025-08-03T10:32:15.000Z"
}
```

---

## âš ï¸ Error Handling

### **Validation Errors**

#### **Missing store_name for specific scan**
```json
{
  "status": "error",
  "message": "store_name is required when scan_type is 'specific'",
  "valid_scan_types": ["latest", "all", "specific"]
}
```

#### **Invalid scan_type**
```json
{
  "status": "error",
  "message": "Invalid scan_type: invalid_type",
  "valid_scan_types": ["latest", "all", "specific"]
}
```

### **Execution Errors**
```json
{
  "status": "error",
  "message": "Pipeline execution failed: Database connection timeout",
  "timestamp": "2025-08-03T10:35:22.456Z"
}
```

---

## ğŸ”„ Pipeline Execution Flow

### **Step-by-Step Process**

#### **1. Request Validation**
- Validate client_id exists
- Check scan_type is valid ("latest", "all", "specific")
- Ensure store_name provided for specific scans
- Verify table names format (if provided)

#### **2. Discovery Phase** (if `include_discovery = true`)
```
ğŸ” Discovery Agent Execution:
â”œâ”€â”€ Scan GCP projects for new resources
â”œâ”€â”€ Discover BigQuery datasets
â”œâ”€â”€ Find GCS buckets
â”œâ”€â”€ Identify PostgreSQL/MySQL databases
â””â”€â”€ Update data_stores table with new sources
```

#### **3. Scanning Phase** (based on `scan_type`)

##### **Latest Database Scan**
```
ğŸ” Latest Scan Process:
â”œâ”€â”€ Query most recent database from data_stores
â”œâ”€â”€ Get client SDE selections
â”œâ”€â”€ Filter patterns based on SDE preferences
â”œâ”€â”€ Initialize appropriate scanner (GCS/BigQuery/PostgreSQL/MySQL)
â””â”€â”€ Execute scan with filtered patterns
```

##### **All Databases Scan**
```
ğŸ” All Databases Scan Process:
â”œâ”€â”€ Query all client databases from data_stores
â”œâ”€â”€ Get client SDE selections (once)
â”œâ”€â”€ Filter patterns based on SDE preferences
â”œâ”€â”€ For each database:
â”‚   â”œâ”€â”€ Initialize appropriate scanner
â”‚   â”œâ”€â”€ Execute scan with filtered patterns
â”‚   â””â”€â”€ Aggregate findings
â””â”€â”€ Combine results from all sources
```

##### **Specific Database Scan**
```
ğŸ” Specific Scan Process:
â”œâ”€â”€ Validate store_name exists for client
â”œâ”€â”€ Get client SDE selections
â”œâ”€â”€ Filter patterns based on SDE preferences
â”œâ”€â”€ Initialize scanner for specific source type
â”œâ”€â”€ Apply table filtering (if specified)
â””â”€â”€ Execute targeted scan with filtered patterns
```

#### **4. AI Analysis Phase** (if `include_ai_analysis = true`)
```
ğŸ¤– AI Analysis Process:
â”œâ”€â”€ Analyze findings distribution
â”œâ”€â”€ Assess data sensitivity levels
â”œâ”€â”€ Generate compliance recommendations
â”œâ”€â”€ Create risk assessment summary
â””â”€â”€ Provide actionable insights
```

---

## ğŸ¯ Usage Scenarios

### **Development & Testing**
```bash
# Quick test scan
curl -X POST "http://localhost:8000/run-pipeline" \
  -H "Content-Type: application/json" \
  -d '{
    "client_id": "test-client",
    "scan_type": "latest",
    "include_discovery": false,
    "include_ai_analysis": false
  }'
```

### **Production Monitoring**
```bash
# Daily comprehensive scan
curl -X POST "http://localhost:8000/run-pipeline" \
  -H "Content-Type: application/json" \
  -d '{
    "client_id": "prod-client",
    "scan_type": "all",
    "include_discovery": true,
    "include_ai_analysis": true
  }'
```

### **Incident Investigation**
```bash
# Targeted investigation
curl -X POST "http://localhost:8000/run-pipeline" \
  -H "Content-Type: application/json" \
  -d '{
    "client_id": "security-team",
    "scan_type": "specific",
    "store_name": "suspicious-bucket",
    "tables": ["user_data", "transactions"],
    "include_discovery": true,
    "include_ai_analysis": true
  }'
```

---

## ğŸ“ˆ Performance & Benefits

### **Scan Type Comparison**

| Scan Type | Speed | Coverage | Use Case | Resource Usage |
|-----------|--------|----------|----------|----------------|
| **Latest** | âš¡ Fast | ğŸ¯ Focused | Daily monitoring | ğŸ’° Low |
| **All** | ğŸŒ Slower | ğŸŒ Complete | Compliance audit | ğŸ’°ğŸ’° High |
| **Specific** | âš¡ Fast | ğŸ¯ Targeted | Investigation | ğŸ’° Low |

### **SDE Filtering Impact**

| Client SDE Count | Pattern Reduction | Performance Gain | Use Case |
|------------------|-------------------|------------------|----------|
| **3 SDEs** | 88% fewer patterns | 3-5x faster | Focused compliance |
| **8 SDEs** | 68% fewer patterns | 2-3x faster | Balanced coverage |
| **No SDEs** | 0% reduction | Baseline speed | Complete discovery |

### **Modular Execution Benefits**
- âœ… **Skip Discovery**: 30-50% time reduction when sources are known
- âœ… **Skip AI Analysis**: 20-30% time reduction for batch processing
- âœ… **Table Filtering**: 40-70% time reduction for specific investigations
- âœ… **SDE Filtering**: 60-90% pattern processing reduction

---

## ğŸ”§ Integration Examples

### **Automated Workflows**

#### **Daily Security Scan**
```python
import requests

def daily_security_scan(client_id):
    response = requests.post("http://localhost:8000/run-pipeline", json={
        "client_id": client_id,
        "scan_type": "latest",
        "include_discovery": True,
        "include_ai_analysis": True
    })
    return response.json()
```

#### **Weekly Compliance Audit**
```python
def weekly_compliance_audit(client_id):
    response = requests.post("http://localhost:8000/run-pipeline", json={
        "client_id": client_id,
        "scan_type": "all",
        "include_discovery": True,
        "include_ai_analysis": True
    })
    return response.json()
```

#### **On-Demand Investigation**
```python
def investigate_resource(client_id, resource_name, tables=None):
    request_data = {
        "client_id": client_id,
        "scan_type": "specific",
        "store_name": resource_name,
        "include_discovery": True,
        "include_ai_analysis": True
    }
    if tables:
        request_data["tables"] = tables
    
    response = requests.post("http://localhost:8000/run-pipeline", json=request_data)
    return response.json()
```

---

## ğŸ‰ Key Advantages

### **ğŸ¯ Flexibility**
- Choose exactly what to scan (latest/all/specific)
- Control pipeline steps (discovery/scan/analysis)
- Filter by tables for granular control

### **âš¡ Performance**
- SDE filtering reduces processing by 60-90%
- Modular execution saves 20-50% execution time
- Targeted scans focus resources where needed

### **ğŸ”§ Control**
- Fine-grained configuration options
- Real-time SDE preference application
- Comprehensive execution reporting

### **ğŸ›¡ï¸ Security**
- Client-isolated operations
- Configurable sensitivity scanning
- Compliance-focused analysis

### **ğŸ“Š Insights**
- Detailed execution summaries
- AI-enhanced recommendations
- Actionable security insights

---

**Pipeline Version:** 2.0  
**Enhanced Features:** Configurable scan types, modular execution, SDE integration  
**Last Updated:** August 3, 2025  
**Compatibility:** Backward compatible with v1.0 pipeline requests
